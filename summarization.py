# -*- coding: utf-8 -*-
"""Copy of summarization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D_spggxXVhm8YECWRvtKjokfEtOdVGqA
"""

!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

!nvidia-smi

from transformers import pipeline, set_seed

import matplotlib.pyplot as plt
from datasets import load_dataset
import pandas as pd
from datasets import load_dataset, load_metric

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

import nltk
from nltk.tokenize import sent_tokenize

from tqdm import tqdm
import torch

import tensorflow as tf

nltk.download("punkt")

device = "cuda" if torch.cuda.is_available() else "cpu"
device

model_ckpt = "google/pegasus-cnn_dailymail"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)

#Load data

dataset_samsum = load_dataset("samsum")

dataset_samsum

split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]
split_lengths

print(f"Features: {dataset_samsum['train'].column_names}")

print("\nDialogue:")

print(dataset_samsum["test"][1]["dialogue"])

print("\nSummary:")

print(dataset_samsum["test"][1]["summary"])

# Evaluating PEGASUS on SAMSum

dataset_samsum['test'][0]['dialogue']

print("\nSummary:")

print(dataset_samsum["test"][0]["summary"])

pipe = pipeline('summarization', model = model_ckpt )

pipe_out = pipe(dataset_samsum['test'][0]['dialogue'] )

print(pipe_out)

print(pipe_out[0]['summary_text'].replace(" .", ".\n"))

def generate_batch_sized_chunks(list_of_elements, batch_size):
    """split the dataset into smaller batches that we can process simultaneously
    Yield successive batch-sized chunks from list_of_elements."""
    for i in range(0, len(list_of_elements), batch_size):
        yield list_of_elements[i : i + batch_size]

def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,
                               batch_size=16, device=device,
                               column_text="article",
                               column_summary="highlights"):
    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)):

        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,
                        padding="max_length", return_tensors="pt")

        summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                         attention_mask=inputs["attention_mask"].to(device),
                         length_penalty=0.8, num_beams=8, max_length=128)
        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''

        # Finally, we decode the generated texts,
        # replace the  token, and add the decoded texts with the references to the metric.
        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,
                                clean_up_tokenization_spaces=True)
               for s in summaries]

        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]


        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    #  Finally compute and return the ROUGE scores.
    score = metric.compute()
    return score

rouge_metric = load_metric('rouge')

score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)

rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )

pd.DataFrame(rouge_dict, index = ['pegasus'])

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )

    return {
        'input_ids' : input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids']
    }

dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)

dataset_samsum_pt['train'][0]

from transformers import DataCollatorForSeq2Seq

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Colab Notebooks

!pip install accelerate

pip install transformers[torch]

!pip install accelerate -U

!pip install transformers==4.17

from transformers import TrainingArguments, Trainer
trainer_args = TrainingArguments(
    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,
    per_device_train_batch_size=1, per_device_eval_batch_size=1,
    weight_decay=0.01, logging_steps=10,
    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,
    gradient_accumulation_steps=16
)

trainer = Trainer(model=model_pegasus, args=trainer_args,
                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                  train_dataset=dataset_samsum_pt["train"],
                  eval_dataset=dataset_samsum_pt["validation"])

trainer.train()

score = calculate_metric_on_test_ds(
    dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'
)

rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )

pd.DataFrame(rouge_dict, index = [f'pegasus'] )

## Save model
model_pegasus.save_pretrained("pegasus-samsum-model")

## Save tokenizer
tokenizer.save_pretrained("tokenizer")

tokenizer = AutoTokenizer.from_pretrained("tokenizer")

dataset_samsum = load_dataset("samsum")

sample_text = dataset_samsum["test"][0]["dialogue"]

reference = dataset_samsum["test"][0]["summary"]

sample_text

reference

gen_kwargs = {"length_penalty": 0.8, "num_beams":8, "max_length": 128}

pipe = pipeline("summarization", model="pegasus-samsum-model",tokenizer=tokenizer)

print("Dialogue:")
print(sample_text)


print("\nReference Summary:")
print(reference)

print("\nModel Summary:")
print(pipe(sample_text, **gen_kwargs)[0]["summary_text"])

print (sample_text)

my_text = "In the heart of a bustling city, amidst the towering skyscrapers and bustling streets, there lies a hidden gemâ€”a quaint bookstore known as 'Whispers of the Past.' Its shelves are filled with stories from centuries ago, waiting to be discovered by curious minds. The aroma of old books mingles with the scent of freshly brewed coffee, creating a cozy atmosphere that welcomes book lovers from near and far. The owner, an elderly man named Mr. Hawthorne, can often be found behind the counter, lost in a book of poetry or engaging in lively discussions with customers about their latest literary finds. The bookstore is a sanctuary for those seeking solace from the chaos of city life, a place where time seems to slow down, allowing one to immerse themselves in the magic of storytelling. As dusk falls and the city lights twinkle outside, the bookstore takes on a magical aura. The books seem to come alive, their pages whispering tales of love, adventure, and mystery. It's said that if you listen closely, you can hear the echoes of the past within these walls, stories of lives lived and dreams pursued. In this haven of literature, where the past meets the present, each book holds the promise of a new adventure, a new world to explore. 'Whispers of the Past' is more than just a bookstore; it's a gateway to the imagination, a place where stories come to life, and the love for books is celebrated in all its glory."

print(pipe(my_text, **gen_kwargs)[0]["summary_text"])